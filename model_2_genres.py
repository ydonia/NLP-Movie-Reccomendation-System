# -*- coding: utf-8 -*-
"""Model 2 - Genres.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BQZPq4U8m-rgrSOz37tdGsxlpJ45j5pP

2nd model is sentiment analysis on what kind of movie it is, meaning comedy/horror/etc. This would be accomplished by training it on descriptive data, which we would need to find a dataset for, and then using the "movie description" on Rotten Tomatoes to determine what type of movie it is.

1. Find a dataset which evaluates some amount of text, and can classify it based on genre. An ideal one would be movie genres, but alternatively can be books or anything else. It would have a small paragraph describing the movie/book/etc, and based on keywords within it, it can classify it by genre.
2. Create a sentiment analysis neural network in any library, and then train it using this dataset. Output will be % confidence in each category, for example comedy 78%, horror 2%, action 35%, etc.
3. Test it on the Rotten Tomatoes dataset, on the category called "Movie Description" to evaluate performance. https://www.kaggle.com/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset?select=rotten_tomatoes_movies.csv

-- Salman, needs more ppl

Potential dataset: http://www.cs.cmu.edu/~ark/personas/

Use this to train the model. May need to take out some of the columns if you dont want to use all the different genres.
# Load data
"""

import pandas as pd
import numpy as np
import nltk
import os 
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

"""# Preprocessing

The steps below can be skipped if you have the /content/cleaned_IMDB_Dataset.csv loaded in below after the Preprocessing section as that csv file is preprocessed to save time from preprocessing on each run.
"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/cleaned_IMDB_Dataset.csv")
csvDF = pd.DataFrame(data, columns=["genre","description"])
csvDF = csvDF.dropna()
genreList = csvDF["genre"].tolist()
descriptionList = csvDF["description"].tolist()

csvDF['new_description'] = csvDF['description'].apply(lambda x: " ".join(x.lower() for x in x.split()))
csvDF['new_description'] = csvDF['new_description'].str.replace('[^\w\s]','') # Replaces all puncutation essentially
stop = stopwords.words('english') # creates stop words
processed_reviews = []
for review in csvDF['new_description']: # for each review, tokenize, and remove stopwords
  text_token = word_tokenize(review)
  text_token_nosw = [word for word in text_token if word not in stopwords.words('english')]
  processed_reviews.append(" ".join(text_token_nosw)) # join the tokenized & stop word removed words and append to new list
csvDF['new_description'] = processed_reviews # assign that list to that data frame column of 'new description'
print(csvDF.head())

totalGenreList = []
for genre in csvDF['genre']:
  genreList = genre.split(', ')
  for newGenre in genreList:
    if newGenre not in csvDF.columns:
      csvDF[newGenre] = 0
    if newGenre not in totalGenreList:
      totalGenreList.append(newGenre)
print(csvDF.columns)

pd.set_option('display.max_columns', None)
print(csvDF['genre'])

for index_row, row_series in csvDF.iterrows():
  genreList = csvDF.at[index_row, 'genre']
  genreList = genreList.split(', ')
  for genre in genreList:
    csvDF.at[index_row,genre] = 1
  for genre in totalGenreList:
    if csvDF.at[index_row, genre] != 1:
      csvDF.at[index_row, genre] = 0
  
print("NEXT")
print(csvDF.head())

"""#Processed Contents"""

#csvDF.to_csv('cleaned_IMDB_Dataset.csv') # this lines saves the contents
#print("saved")

csvDF = pd.read_csv("/content/drive/MyDrive/AIM Fall 2021 Project - Hemal, Youssef, Salman/cleaned_IMDB_Dataset.csv")
#/content/drive/MyDrive/AIM Fall 2021 Project - Hemal, Youssef, Salman/cleaned_IMDB_Dataset.csv
pd.set_option('display.max_columns', None)
print(csvDF.head())

"""#LSTM

import the stuff we need
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
# %matplotlib inline

"""check that we have the files we need"""

import os
print(os.listdir("../content"))

"""Genres we should maybe remove: 
- Biography, History, Western, War?, Sport, Musical, Music, Film-Noir(same as mystery), documentary, Reality-TV, News
"""

train = pd.read_csv("/content/cleaned_IMDB_Dataset.csv") # read the csv file to dataframe

#/content/drive/MyDrive/AIM Fall 2021 Project - Hemal, Youssef, Salman/cleaned_IMDB_Dataset.csv
removedGenres = ['Biography','History','Western','War','Sport','Music', 'Musical','Film-Noir','Documentary','Reality-TV','News','Crime','Animation']
train = train.drop(removedGenres, axis=1) 
train.head()

for genre in removedGenres:
  train = train[train['genre'] != genre]
train.head(20)

for genre in removedGenres:
  train = train[~train['genre'].str.contains(genre)]
train.head(20)

"""make X_train the movie description column

make Y_train all the colums from first genre to last genre
"""

processed_reviews = []
wordAmt = 0
for review in train['new_description']: # for each review, tokenize, and remove stopwords
  strReview = str(review)
  strippedWords = strReview.strip()
  wordAmt += len(strippedWords.split(" "))
  processed_reviews.append(strippedWords) # join the tokenized & stop word removed words and append to new list
train['new_description'] = processed_reviews # assign that list to that data frame column of 'new description'
print(wordAmt)

X_train = train['new_description']
y_train = train.iloc[:, 4:]

"""rough outline for training model"""

# tokenizer
from keras.preprocessing.text import Tokenizer

numGenres = 12 # removed 3 more so not 15 anymore just 12
num_words = 75000
tokenizer = Tokenizer(num_words= num_words)
tokenizer.fit_on_texts(X_train)
X_train_seq = pd.Series(tokenizer.texts_to_sequences(X_train))

X_train_pad = pad_sequences(X_train_seq, maxlen=256)
print(X_train_pad.shape)

# tokenizer
from keras.preprocessing.text import Tokenizer

numGenres = 12 # removed 3 more so not 15 anymore just 12
num_words = 75000
tokenizer = Tokenizer(num_words= num_words)
tokenizer.fit_on_texts(X_train)
X_train_seq = pd.Series(tokenizer.texts_to_sequences(X_train))

X_train_pad = pad_sequences(X_train_seq, maxlen=256)
print(X_train_pad.shape)

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# print(len(y_train))
# print(len(X_train))

model = Sequential()

model.add(Embedding(input_dim=num_words, output_dim=256)) # embedding (needed to load NLP data)
model.add(LSTM(256, return_sequences=True)) # LTSM
model.add(Bidirectional(LSTM(128, return_sequences=True)))
#model.add(LSTM(128, return_sequences=True)) # LTSM added
model.add(LSTM(128, return_sequences=False))
model.add(Dense(64, activation='relu')) # first dense layer
model.add(Dropout(0.1))

model.add(Dense(numGenres, activation= 'sigmoid')) # output layer

model.summary()

model.compile(loss="binary_crossentropy", optimizer='adam', metrics=['accuracy']) # not sure about the details here we need to check

# we'll change all these number later this is just to have an outline
batch_size = 128
epochs = 15
validation_split = 0.01
print(X_train_pad.shape)
print(y_train.shape)

model.fit(x=X_train_pad, y=(y_train), batch_size=batch_size, epochs=epochs, validation_split=validation_split)

model.save('genre.h5')

print(np.round(predictions))

npPredArr = np.array(np.round(predictions))
df = pd.DataFrame(npPredArr)
print(df)

